# Model Selection Strategy: Optimizing Claude Model Usage

**Version:** 1.0
**Last Updated:** 2026-01-17
**Status:** Active

---

## Overview

The supervisor system can use different Claude models strategically to optimize cost and performance:
- **Claude Haiku** - Fast, cheap, simple tasks
- **Claude Sonnet** - Complex thinking, planning, architecture
- **Claude Opus** - Reserved for novel/critical decisions (future use)

**Key insight:** Use the right model for the task complexity.

---

## Model Capabilities & Costs

### Claude Haiku (Fast & Economical)
**Best for:**
- Binary decisions (yes/no, exists/doesn't exist)
- Status polling (check if done, read comments)
- Simple parsing (extract PR URL, read JSON)
- Routine verification (acknowledgment checks)

**Characteristics:**
- âš¡ Fast response time (~1-2s)
- ğŸ’° 60-70% cheaper than Sonnet
- ğŸ¯ Accurate for well-defined tasks
- âš ï¸ Limited complex reasoning

**Cost:** ~$0.25 per million input tokens, ~$1.25 per million output tokens

### Claude Sonnet 4.5 (Balanced - Current Default)
**Best for:**
- Strategic planning and decision-making
- Code analysis and architecture
- Complex multi-step workflows
- Plan evaluation and approval
- Comprehensive verification

**Characteristics:**
- ğŸ§  Strong reasoning capabilities
- âš–ï¸ Balanced cost vs performance
- ğŸ¯ Handles ambiguity well
- âœ… Current supervisor default

**Cost:** ~$3.00 per million input tokens, ~$15.00 per million output tokens

### Claude Opus 4.5 (Most Capable - Reserved)
**Best for:**
- Novel problem solving
- Critical architectural decisions
- Highly complex multi-file refactoring
- Security-critical evaluations

**Characteristics:**
- ğŸš€ Best reasoning and analysis
- ğŸ’ Highest cost
- ğŸ”¬ Research-level thinking
- â° Slower response time

**Cost:** ~$15.00 per million input tokens, ~$75.00 per million output tokens

---

## Task-to-Model Mapping

### Haiku Tasks (Simple, Fast, Cheap)

#### Verification & Monitoring
- âœ… **verify-scar-start.md** - Check if SCAR acknowledged (binary)
- âœ… **Status polling** - Every 2min check for completion signals
- âœ… **Comment parsing** - Extract PR URLs, branch names from comments
- âœ… **File existence checks** - Verify worktree paths exist
- âœ… **Health checks** - SCAR server status, webhook status

#### Routing & Handoff
- âœ… **Context handoff reading** - Read handoff doc, spawn next agent
- âœ… **Simple routing** - Determine which subagent to spawn
- âœ… **Log parsing** - Extract status from log files

#### Data Extraction
- âœ… **GitHub API calls** - Fetch issue comments, PR status
- âœ… **JSON parsing** - Read workflow-status.yaml, extract fields
- âœ… **Pattern matching** - Find keywords in text (grep-like operations)

**Token estimate:** 3-5K per task
**Frequency:** High (every 2min for monitoring)
**Cost impact:** Massive savings (60-70% cheaper Ã— high frequency)

### Sonnet Tasks (Complex, Strategic)

#### Planning & Architecture
- âœ… **analyze.md** - Codebase analysis and research
- âœ… **create-epic.md** - Strategic feature planning
- âœ… **create-adr.md** - Architectural decision records
- âœ… **plan-feature.md** - Full feature orchestration

#### Supervision & Orchestration
- âœ… **supervise-issue.md** - Full issue lifecycle management
- âœ… **approve-scar-plan.md** - Plan evaluation and validation
- âœ… **verify-scar-phase.md** - Comprehensive build/test verification

#### Complex Decision-Making
- âœ… **Error recovery** - Diagnose failures, determine fix strategy
- âœ… **Conflict resolution** - Handle merge conflicts, test failures
- âœ… **Resource allocation** - Decide parallelization strategy
- âœ… **Quality assessment** - Code review, pattern compliance

**Token estimate:** 15-30K per task
**Frequency:** Lower (per epic, per issue)
**Cost impact:** Higher cost but necessary for quality

### Opus Tasks (Reserved - Future Use)

#### Novel Problem Solving
- âš ï¸ **New technology integration** - First-time framework setup
- âš ï¸ **Complex refactoring** - Multi-repo architectural changes
- âš ï¸ **Security audits** - Critical vulnerability assessment
- âš ï¸ **Performance optimization** - Deep algorithmic improvements

**Token estimate:** 30-50K+ per task
**Frequency:** Rare (major milestones only)
**Cost impact:** Very high, but justified for critical work

---

## Implementation Guide

### How to Specify Model

When spawning subagents with Task tool, use the `model` parameter:

```python
# Haiku for simple tasks
Task(
  subagent_type="Bash",
  model="haiku",  # â† Specify model
  prompt="Check if SCAR acknowledged instruction on issue #42",
  description="Verify SCAR started"
)

# Sonnet (default) for complex tasks
Task(
  subagent_type="general-purpose",
  # No model param = defaults to Sonnet
  prompt="Supervise issue #42 through completion",
  description="Supervise issue #42"
)

# Opus for critical tasks (future)
Task(
  subagent_type="Plan",
  model="opus",  # â† Reserved for critical decisions
  prompt="Design multi-service authentication architecture",
  description="Plan auth architecture"
)
```

### Decision Flowchart

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Need to spawn subagent?         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Is task well-defined       â”‚
    â”‚ with binary/simple output? â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ YES           â”‚ NO
         â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ HAIKU  â”‚      â”‚ Is it       â”‚
    â”‚        â”‚      â”‚ strategic   â”‚
    â”‚ Fast   â”‚      â”‚ planning or â”‚
    â”‚ Cheap  â”‚      â”‚ complex?    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
                       â”‚ YES   â”‚ NO
                       â–¼       â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ SONNET â”‚ â”‚ HAIKU  â”‚
                  â”‚        â”‚ â”‚ or     â”‚
                  â”‚ Complexâ”‚ â”‚ SONNET â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Cost Optimization Strategies

### Already Built In (Epic Sharding)
Your system has 90% token reduction through epic sharding:
- Without sharding: 50K+ tokens (entire codebase)
- With sharding: 2-5K tokens (focused epic)
- **Savings:** 90% reduction = 10x more features per budget

### Model Selection (New Optimization)
Additional 60-70% cost savings on frequent tasks:
- Monitoring runs every 2min
- Status checks run constantly
- Using Haiku instead of Sonnet = 60-70% cheaper
- **Savings:** Massive on high-frequency operations

### Combined Effect
```
Without optimization:
  Task: Monitor issue #42
  Model: Sonnet
  Tokens: 50K (full codebase) Ã— $3/M input = $0.15 per check
  Frequency: Every 2min = 30 checks/hour
  Cost: $4.50/hour monitoring

With epic sharding only:
  Task: Monitor issue #42
  Model: Sonnet
  Tokens: 5K (epic only) Ã— $3/M input = $0.015 per check
  Frequency: Every 2min = 30 checks/hour
  Cost: $0.45/hour monitoring (90% savings)

With epic sharding + Haiku:
  Task: Monitor issue #42
  Model: Haiku
  Tokens: 5K (epic only) Ã— $0.25/M input = $0.00125 per check
  Frequency: Every 2min = 30 checks/hour
  Cost: $0.0375/hour monitoring (99% savings vs original!)
```

**Result:** Nearly free monitoring while preserving Sonnet for complex work.

---

## Migration Strategy

### Phase 1: Update Simple Monitoring (Immediate)
**Target tasks:**
- verify-scar-start.md spawns
- Status polling loops
- Simple health checks

**Implementation:**
1. Update CLAUDE.md with model selection guidance âœ… (Done)
2. Update supervision protocol to use Haiku for simple tasks
3. Test with single issue monitoring
4. Verify cost savings in usage reports

**Expected savings:** 60-70% on monitoring costs

### Phase 2: Audit All Subagent Spawns (Week 1)
**Process:**
1. List all places where Task tool is called
2. Categorize each by complexity
3. Add `model="haiku"` for simple tasks
4. Document decision rationale

**Expected savings:** 40-50% overall cost reduction

### Phase 3: Dynamic Model Selection (Future)
**Vision:**
1. Implement complexity scoring
2. Auto-select model based on task
3. Track quality vs cost metrics
4. ML-based optimization

**Expected savings:** 50-60% with maintained quality

---

## Monitoring & Validation

### Track These Metrics

**Cost Metrics:**
- Total token usage by model (Haiku vs Sonnet vs Opus)
- Cost per task type
- Cost per project
- Monthly trend analysis

**Quality Metrics:**
- Task success rate by model
- False positive rate (incorrect decisions)
- Need for retries by model
- User escalations by model

**Performance Metrics:**
- Response time by model
- Task completion time
- Blocking time reduced

### Success Criteria

**Haiku tasks should achieve:**
- âœ… 95%+ accuracy on binary decisions
- âœ… <5% retry rate
- âœ… <2s response time
- âœ… 60-70% cost savings vs Sonnet

**Sonnet tasks should maintain:**
- âœ… High quality planning and analysis
- âœ… <10% user escalation rate
- âœ… Architectural consistency
- âœ… Code pattern compliance

**Red flags (switch back to Sonnet):**
- âŒ High retry rate (>10%)
- âŒ Frequent errors requiring human intervention
- âŒ Quality degradation (user complaints)
- âŒ Missed critical issues

---

## Examples

### Example 1: Monitoring Issue #42

**Before (Sonnet only):**
```python
# Supervisor spawns monitoring
Task(
  subagent_type="general-purpose",
  # Defaults to Sonnet
  prompt="Monitor issue #42 for completion signals...",
  description="Monitor issue #42"
)
```
**Cost:** ~$0.045 per 2min check = $1.35/hour

**After (Haiku for monitoring):**
```python
# Supervisor spawns monitoring
Task(
  subagent_type="Bash",
  model="haiku",  # â† Fast & cheap for simple checks
  prompt="Monitor issue #42 for completion signals...",
  description="Monitor issue #42"
)
```
**Cost:** ~$0.00375 per 2min check = $0.1125/hour (92% savings!)

### Example 2: Verify SCAR Start

**Before (Sonnet):**
```python
Task(
  subagent_type="Bash",
  prompt="Verify SCAR acknowledged instruction on issue #42...",
  description="Verify SCAR started"
)
```
**Cost:** ~$0.015 per check

**After (Haiku):**
```python
Task(
  subagent_type="Bash",
  model="haiku",  # â† Binary check = perfect for Haiku
  prompt="Verify SCAR acknowledged instruction on issue #42...",
  description="Verify SCAR started"
)
```
**Cost:** ~$0.00125 per check (92% savings)

### Example 3: Plan Evaluation (Keep Sonnet)

```python
# This needs complex reasoning - use Sonnet
Task(
  subagent_type="general-purpose",
  # No model param = defaults to Sonnet (correct!)
  prompt="Evaluate SCAR's implementation plan for security concerns...",
  description="Approve SCAR plan"
)
```
**Cost:** ~$0.30 per evaluation (higher but necessary)

---

## SCAR Limitation

**IMPORTANT:** SCAR currently CANNOT use different models.

**Why:**
- SCAR uses Claude Agent SDK v0.1.57
- SDK doesn't expose model selection parameter
- All SCAR instances use same model (likely Sonnet/Opus)

**Future:**
- Monitor SDK releases for model parameter support
- File location: `/home/samuel/scar/package.json`
- GitHub: anthropics/claude-agent-sdk

**Workaround:**
- Optimize supervisor-side operations (monitoring, verification)
- Let SCAR use best model for implementation
- Focus Haiku savings on supervisor workflows

---

## Best Practices

### DO:
âœ… Use Haiku for frequent, simple tasks (monitoring, status checks)
âœ… Use Sonnet for complex planning and decision-making
âœ… Reserve Opus for critical architectural decisions
âœ… Monitor cost vs quality metrics
âœ… Document model choice rationale
âœ… Test Haiku tasks thoroughly before production use

### DON'T:
âŒ Use Haiku for complex reasoning or planning
âŒ Use Opus for routine tasks
âŒ Skip quality validation when switching to Haiku
âŒ Optimize cost at expense of reliability
âŒ Use Haiku for security-critical decisions
âŒ Assume Haiku can handle ambiguous requirements

---

## Quick Reference

| Task Type | Model | Why | Example |
|-----------|-------|-----|---------|
| SCAR start verification | Haiku | Binary check | "Did SCAR acknowledge?" |
| Status polling | Haiku | Simple parsing | "Extract PR URL from comment" |
| Health checks | Haiku | Binary response | "Is SCAR server up?" |
| Context handoff | Haiku | Simple routing | "Read handoff, spawn next agent" |
| Issue supervision | Sonnet | Complex orchestration | "Manage full issue lifecycle" |
| Plan approval | Sonnet | Security evaluation | "Validate plan has no risks" |
| Build verification | Sonnet | Comprehensive testing | "Run all tests, check quality" |
| Epic creation | Sonnet | Strategic planning | "Design feature breakdown" |
| ADR creation | Sonnet | Architectural thinking | "Document design decisions" |
| Novel architecture | Opus | Deep reasoning | "Design multi-service auth" |

---

## Related Documentation

- **SCAR Integration:** `/home/samuel/supervisor/docs/scar-integration.md`
- **Epic Sharding:** `/home/samuel/supervisor/docs/epic-sharding.md`
- **Subagent Patterns:** `/home/samuel/supervisor/docs/subagent-patterns.md`
- **Context Management:** `/home/samuel/supervisor/docs/supervisor-learnings/learnings/001-subagent-context-handoff.md`

---

**Model selection is a powerful optimization tool - use it wisely to balance cost and quality.**
