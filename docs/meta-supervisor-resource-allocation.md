# Meta-Supervisor: Cross-Project Resource Allocation

**Status:** Proposed Architecture
**Date:** 2026-01-18
**Purpose:** Prevent VM resource exhaustion when multiple projects build in parallel

---

## Problem Statement

**Current Risk:**
- User has 5+ active projects
- Each project supervisor can spawn up to 10 subagents
- If all projects build simultaneously: 50+ agents = VM crash
- No coordination between project supervisors

**Need:**
- Central resource management
- Dynamic allocation of "implementation slots"
- VM health monitoring
- Intelligent scheduling across projects

---

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Meta-Supervisor (Root)                     ‚îÇ
‚îÇ  Location: /home/samuel/supervisor/ (CLAUDE.md)             ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Responsibilities:                                           ‚îÇ
‚îÇ  - Monitor VM resources (CPU, RAM, disk, load)              ‚îÇ
‚îÇ  - Manage global implementation slot pool                   ‚îÇ
‚îÇ  - Allocate slots to project supervisors on request         ‚îÇ
‚îÇ  - Kill runaway processes if VM degraded                    ‚îÇ
‚îÇ  - Report cross-project status                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ                   ‚îÇ
        ‚ñº                   ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Consilio   ‚îÇ   ‚îÇ OpenHorizon ‚îÇ   ‚îÇ   Odin      ‚îÇ
‚îÇ  Supervisor ‚îÇ   ‚îÇ  Supervisor ‚îÇ   ‚îÇ  Supervisor ‚îÇ
‚îÇ             ‚îÇ   ‚îÇ             ‚îÇ   ‚îÇ             ‚îÇ
‚îÇ  Requests:  ‚îÇ   ‚îÇ  Requests:  ‚îÇ   ‚îÇ  Requests:  ‚îÇ
‚îÇ  5 slots    ‚îÇ   ‚îÇ  3 slots    ‚îÇ   ‚îÇ  2 slots    ‚îÇ
‚îÇ             ‚îÇ   ‚îÇ             ‚îÇ   ‚îÇ             ‚îÇ
‚îÇ  Allocated: ‚îÇ   ‚îÇ  Allocated: ‚îÇ   ‚îÇ  Allocated: ‚îÇ
‚îÇ  5 slots ‚úÖ ‚îÇ   ‚îÇ  3 slots ‚úÖ ‚îÇ   ‚îÇ  2 slots ‚úÖ ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                  ‚îÇ                 ‚îÇ
       ‚ñº                  ‚ñº                 ‚ñº
   5 PIV agents      3 PIV agents      2 PIV agents

Total: 10/20 slots used (10 remaining)
```

---

## Implementation

### 1. Global Slot Pool

**File:** `/home/samuel/supervisor/.resource-manager/slots.yaml`

```yaml
# Global configuration
max_slots: 20  # Total implementation agents allowed
slot_allocation:
  consilio: 0
  openhorizon: 0
  health-agent: 0
  odin: 0
  quiculum-monitor: 0

# VM thresholds (when to reduce slots)
vm_limits:
  max_cpu_percent: 85
  max_memory_percent: 90
  max_load_average: 16.0  # For 8-core VM

# Current VM state (updated every 30sec)
vm_state:
  cpu_percent: 45.2
  memory_percent: 62.1
  load_average: 4.3
  disk_usage_percent: 58
  health: "healthy"  # healthy | degraded | critical
```

### 2. Slot Request Protocol

**Project Supervisor wants to build 5 features:**

```bash
# 1. Request slots from Meta-Supervisor
supervisor-request-slots consilio 5

# Meta-Supervisor checks:
# - Current slot usage (10/20 used)
# - VM health (healthy)
# - Priority (FIFO or weighted)

# 2. Allocate slots
# Updates slots.yaml:
#   consilio: 5

# 3. Project supervisor spawns agents
# Spawns 5 PIV subagents in parallel

# 4. On completion, release slots
supervisor-release-slots consilio 5

# Updates slots.yaml:
#   consilio: 0
```

### 3. VM Health Monitoring

**Meta-Supervisor runs continuous monitoring loop:**

```bash
# Script: /home/samuel/supervisor/.resource-manager/monitor-vm.sh

#!/bin/bash
while true; do
  # Get VM metrics
  CPU=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)
  MEM=$(free | grep Mem | awk '{print ($3/$2) * 100}')
  LOAD=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $1}' | tr -d ',')
  DISK=$(df / | tail -1 | awk '{print $5}' | tr -d '%')

  # Update slots.yaml with current state
  yq eval ".vm_state.cpu_percent = $CPU" -i slots.yaml
  yq eval ".vm_state.memory_percent = $MEM" -i slots.yaml
  yq eval ".vm_state.load_average = $LOAD" -i slots.yaml
  yq eval ".vm_state.disk_usage_percent = $DISK" -i slots.yaml

  # Determine health
  if (( $(echo "$CPU > 85" | bc -l) )) || (( $(echo "$MEM > 90" | bc -l) )); then
    yq eval '.vm_state.health = "degraded"' -i slots.yaml
    # Trigger slot reduction
    reduce-slots-emergency
  elif (( $(echo "$LOAD > 16" | bc -l) )); then
    yq eval '.vm_state.health = "critical"' -i slots.yaml
    # Kill lowest priority agents
    kill-low-priority-agents
  else
    yq eval '.vm_state.health = "healthy"' -i slots.yaml
  fi

  sleep 30
done
```

### 4. Intelligent Scheduling Strategies

**Strategy A: First-Come-First-Served (Simple)**
```
Consilio requests 8 slots
‚Üí Allocate 8 (if available)

OpenHorizon requests 10 slots
‚Üí Only 12 available
‚Üí Allocate 10 (2 slots remaining)

Odin requests 5 slots
‚Üí Only 2 available
‚Üí Queue request, notify user: "Waiting for slots (3 agents ahead)"
```

**Strategy B: Fair Share (Balanced)**
```
3 projects request slots simultaneously:
- Consilio: wants 10
- OpenHorizon: wants 8
- Odin: wants 6

Total requested: 24
Total available: 20

Allocate proportionally:
- Consilio: 8 slots (10/24 * 20)
- OpenHorizon: 7 slots (8/24 * 20)
- Odin: 5 slots (6/24 * 20)
```

**Strategy C: Priority-Based (User Defined)**
```yaml
# User sets priorities in supervisor/config.yaml
project_priorities:
  consilio: 10      # Highest priority (production app)
  openhorizon: 8    # High priority
  health-agent: 5   # Medium
  odin: 5           # Medium
  quiculum-monitor: 3  # Low (experimental)

# Allocation favors higher priority projects
```

### 5. User-Facing Commands

**Meta-Supervisor provides these commands:**

```bash
# Show resource usage across all projects
/resource-status

Output:
üìä VM Resource Status

üñ•Ô∏è  VM Health: Healthy
   CPU: 45.2% (of 85% limit)
   RAM: 62.1% (of 90% limit)
   Load: 4.3 (of 16.0 limit)
   Disk: 58%

üéØ Implementation Slots: 10/20 used

Active Projects:
   Consilio:     5 agents (Features: Auth, Dashboard, API, Tests, Deploy)
   OpenHorizon:  3 agents (Features: Landing, Contact, Blog)
   Odin:         2 agents (Features: Parser, Validator)

Queued Projects:
   Health-Agent: Waiting for 4 slots

Estimated completion: 15 minutes


# Check specific project
/resource-status consilio

Output:
üìä Consilio Resource Usage

Allocated Slots: 5/20 (25%)

Active Agents:
   1. Feature/auth - 60% complete (Phase 3/4)
   2. Feature/dashboard - 80% complete (Phase 4/4)
   3. Feature/api - 40% complete (Phase 2/4)
   4. Feature/tests - 90% complete (Running validations)
   5. Feature/deploy - 20% complete (Phase 1/4)

ETA: 12 minutes


# Force kill all agents for a project (emergency)
/resource-kill consilio

Output:
‚ö†Ô∏è  Killing all Consilio agents...
   5 agents terminated
   5 slots released

Reason: User requested


# Adjust global slot limit
/resource-set-max 30

Output:
‚úÖ Max slots increased: 20 ‚Üí 30
   10 additional slots available
   Queued projects can now proceed
```

---

## Decision Tree: When to Use Resource Manager

### Use Resource Manager If:

‚úÖ **Multiple active projects** (3+ projects building simultaneously)
‚úÖ **Limited VM resources** (8-16 GB RAM, 4-8 CPU cores)
‚úÖ **High parallelism** (each project wants 5+ agents)
‚úÖ **Production VM** (can't afford crashes)
‚úÖ **Multiple users** (different people building different projects)

### Skip Resource Manager If:

‚ùå **Single project focus** (only work on one project at a time)
‚ùå **Powerful VM** (64+ GB RAM, 16+ cores)
‚ùå **Low parallelism** (1-2 agents per project max)
‚ùå **Development only** (OK if VM crashes occasionally)

---

## Your VM Specs

**Check your current VM:**
```bash
# CPU cores
nproc
# Output: 8 (example)

# RAM
free -h
# Output: Total: 16Gi

# Current load
uptime
# Output: load average: 2.3, 1.8, 1.5
```

**Recommended slot limits based on VM size:**

| VM Size | CPU Cores | RAM | Max Slots | Max per Project |
|---------|-----------|-----|-----------|-----------------|
| Small   | 2-4       | 4-8 GB | 5-8 | 2-3 |
| Medium  | 4-8       | 8-16 GB | 10-15 | 3-5 |
| Large   | 8-16      | 16-32 GB | 20-30 | 5-10 |
| XLarge  | 16+       | 32+ GB | 40-50 | 10-15 |

---

## Implementation Phases

### Phase 1: Basic Resource Manager (MVP)
- Global `slots.yaml` file
- Simple FIFO allocation
- Manual slot requests
- Basic VM monitoring

### Phase 2: Automated Allocation
- Project supervisors auto-request slots
- Auto-release on completion
- Health-based slot reduction

### Phase 3: Advanced Features
- Priority-based allocation
- Predictive scheduling (estimate completion times)
- Auto-scaling (increase slots when VM healthy)
- Per-project resource usage history

---

## Answer: Is This Overkill?

**For your use case: NO, this is necessary!**

**Why:**
- ‚úÖ You have 5+ active projects
- ‚úÖ You want maximum parallelism (10+ agents)
- ‚úÖ VM resources are finite
- ‚úÖ You're a non-coder (can't manually debug VM crashes)

**The resource manager ensures:**
- ‚úÖ VM never crashes from too many agents
- ‚úÖ Fair allocation across projects
- ‚úÖ Visibility into what's running where
- ‚úÖ Automatic recovery from resource exhaustion

**Complexity level:** Medium
- Simple version (Phase 1): ~2-3 days to implement
- Full version (Phase 3): ~1-2 weeks

**Recommendation:** Start with Phase 1 (basic resource manager), add automation later as needed.

---

**Next Steps:**

1. Check your VM specs (CPU, RAM)
2. Decide on max_slots limit
3. Choose allocation strategy (FIFO vs Fair Share vs Priority)
4. Implement Phase 1 MVP
5. Test with 2-3 projects building in parallel
6. Add automation (Phase 2) based on results
